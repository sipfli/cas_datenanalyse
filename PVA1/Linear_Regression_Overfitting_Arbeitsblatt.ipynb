{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lineare Regession und Overfitting\n",
    "\n",
    "Hier untersuchen wir einen einfachen Datensatz zur Regression, und versuchen, uns einige Begriffe klar zu machen, insbesondere: \n",
    "- Zielfunktion \n",
    "- die Modellkomplexität\n",
    "- das Rauschen \n",
    "- Menge der Verfügbaren Trainingsdaten\n",
    "\n",
    "Zudem wollen wir beim Fitten herauszufinden, welche Art von Struktur der Fit-Algorithmus berücksichtigt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "#Dieses Skript ist lauffähig unter sklearn Version 0.18.1\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Wir importieren \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#seaborn enthält hübschere Defaults für Matplotlibplots etc.\n",
    "import seaborn as sns\n",
    "sns.set_color_codes('muted')\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wir ziehen n_samples Datenpunkte\n",
    "n_samples = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dies ist die Zielfunktion. Normalerweise ist sie nicht bekannt!\n",
    "true_fun = lambda X: np.cos(1.5 * np.pi * X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wieviel Rauschen sollen die Daten haben?\n",
    "noise=0.1\n",
    "X_train = np.sort(np.random.rand(n_samples,1))\n",
    "y_train = true_fun(X_train) + np.random.randn(n_samples,1) * noise\n",
    "plt.scatter(X_train,y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lineare Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lineare Regression an die Daten (fit einer Geraden)\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die Regressionskoeffizienten\n",
    "linear_regression.coef_,linear_regression.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=np.linspace(0,1,101).reshape(-1,1)\n",
    "y_test=true_fun(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe:** Bestimmen Sie nun die Genauigkeit des `linear_regression` Klassifikators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train,y_train,label='Trainings-Datenpunkte auf Zielfunktion')\n",
    "plt.plot(X_test,y_test,'--',label='Zielfunktion');\n",
    "plt.plot(X_test,yhat_test,label='Vorhersage (auf Testdaten)\\ndurch Polynom vom Grad 1');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PolynomialFeatures und Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun fitten wir Polynome höheren Grades an die Daten. Dies bedeutet, wir fitten nicht nur an das Feature $x$, sondern auch an $x^2, x^3,\\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nun wollen wir einen linearen Fit, aber nicht nur an das Feature x, sondern x,x^2,...,x^d\n",
    "just_to_try_degree = 3\n",
    "just_to_try_NSamples = 3\n",
    "polynomial_features = PolynomialFeatures(degree=just_to_try_degree)\n",
    "polynomial_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Was tut polynomial_features?\n",
    "just_to_try_X=np.array(np.linspace(0,1,just_to_try_NSamples)).reshape(-1,1);\n",
    "\n",
    "#für just_to_try_degree=3\n",
    "polynomial_features.fit_transform(just_to_try_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe:** Erklären Sie diesen Output. Sagen Sie den Output vorher für z.B. für einen Input von [2,5,-3]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter: Bitte damit experimentieren!\n",
    "#Insbesondere Werte \n",
    "noise=10.0\n",
    "#(0,10),(1,10),(3,10),(8,10),(20,10) mit noise=0.0 und noise 0.3\n",
    "just_to_try_degree = 20\n",
    "just_to_try_NSamples = 10\n",
    "\n",
    "\n",
    "#erstelle Trainingsdatensatz (in der Praxis oft teuer!)\n",
    "just_to_try_X=np.random.rand(just_to_try_NSamples).reshape(-1,1)\n",
    "just_to_try_y=true_fun(just_to_try_X) +\\\n",
    "    np.random.randn(just_to_try_NSamples,1) * noise\n",
    "\n",
    "\n",
    "#Lerne das Modell\n",
    "polynomial_features = PolynomialFeatures(degree=just_to_try_degree)\n",
    "pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "pipeline.fit(just_to_try_X,just_to_try_y)\n",
    "just_to_try_yhat=pipeline.predict(just_to_try_X)\n",
    "\n",
    "#Plots\n",
    "Xline=np.linspace(0,1,100)\n",
    "yline=true_fun(Xline)\n",
    "plt.plot(Xline,yline,'--',label='Zielfunktion')\n",
    "\n",
    "plt.scatter(just_to_try_X,true_fun(just_to_try_X),label='Trainings-Datenpunkte auf Zielfunktion');\n",
    "plt.scatter(just_to_try_X,just_to_try_yhat,label='Vorhersage (auf Trainingssatz)\\ndurch Polynom vom Grad {0}'.format(just_to_try_degree));\n",
    "plt.title('Anzahl Datenpunkte: {0}'.format(just_to_try_NSamples))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_to_try_X.shape,just_to_try_y.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUFGABE\n",
    "Finden Sie Polynomgrade, welche ein Underfitting und ein Overfitting darstellen. Geben Sie auch Ihre persönliche optimale Wahl (dazwischen?) an.  \n",
    "\n",
    "- Experimentieren Sie mit den Parametern in der folgenden Zelle. Wie hängen der Trainings- und Cross-Validation-Fehler von diesen Parametern ab?  \n",
    "- Wagen Sie eine Verallgemeinerung Ihrer Befunde! Was könnten Sie z.B. für eine Klassifikationsaufgabe mit Entscheidungsbäumen bedeuten?  \n",
    "- Wie verhalten sich der Ridge- und der Lasso-Regressor? Wie unterscheiden Sie sich vom normalen least-squares-Klassifikator? Finden Sie Parameterwerte, für welche Sie mit diesen Regressoren ebenfalls gute Resultate erhalten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "degrees = [1,2,10,200] #z.B. [1,2,3,10,200]\n",
    "\n",
    "# Wie ist die Abhängigkeit vom Rauschen?\n",
    "noise=0.1 #0.1\n",
    "\n",
    "# Wir ziehen n_samples Datenpunkte\n",
    "n_samples = 50  #20,200,2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.sort(np.random.rand(n_samples,1))\n",
    "y_train = true_fun(X_train) + np.random.randn(n_samples,1) * noise\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "X_test = np.linspace(0, 1, 100).reshape(-1,1)\n",
    "y_test = true_fun(X_test)\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
    "                                             include_bias=False)\n",
    "    #regressor = Ridge(alpha=)\n",
    "    #regressor = Lasso(alpha=)\n",
    "    regressor = LinearRegression()\n",
    "    \n",
    "    \n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", regressor)])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, X_train, y_train,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    train_score = mean_squared_error(y_train,pipeline.predict(X_train))\n",
    "\n",
    "    plt.scatter(X_train, y_train,c='b', label=\"Samples\")\n",
    "    plt.plot(X_test, pipeline.predict(X_test), c='r', label=\"Model\")\n",
    "    plt.plot(X_test, y_test,c='k', label=\"True function\")\n",
    "    \n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\\nCross-Val MSE = {:.2g}(+/- {:.2g})\\ntraining MSE ={:.2f}\".format(\n",
    "        degrees[i], -scores.mean(), scores.std(),train_score))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle: Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
